# Chapter 3 - Simplifying Data Processing Through Snowpark

This Chapter focuses on loading diverse datasets into a Snowflake schema and covers how to leverage the Snowpark API for dataset exploration, transformation, and powerful analysis, including various aggregation methodologies.

## Contents

Within this chapter, you'll find three IPython notebooks, each dedicated to distinct facets of the chapter's contents.

| File or Folder Name         |  Topics Covered                   |
| ----------------------------|  -------------------------------- |
| Chapter_3_data_load.ipynb   |<p>Loading CSV </p><p>Loading JSON</p><p> Loading Parquet </p> <p> Loading Images </p> <p> Dynamically Reading Multiple Files </p>|
| Chapter_3_explore_transform.ipynb|  <p> Reading In Tables </p> <p> Count Vs Collect </p>  <p> Dataset Filtering & Subsetting </p>  <p> Joins & Unions </p> |
| Chapter_3_agg_analysis.ipynb |  <p> Data Description </p> <p> Pivoting & Crosstabs </p>  <p> Dealing with Duplicate & Missing Values </p>  <p> Correlation & Covariance </p> <p> Sampling </p> <p> Charting </p>|  

## Datasets

The dataset folder in the repository contains all required dataset files for this chapter

* [purchase_history.csv](../datasets/purchase_history.csv)
* [marketing_additional.csv](../datasets/marketing_additional.csv)
* [campaign_info.json](../datasets/campaign_info.json)
* [complain_info.parquet](../datasets/complain_info.parquet)
* [sample_images folder](../datasets/sample_images)




